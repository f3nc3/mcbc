{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSkv3mvJH4o1",
    "outputId": "1a16e65b-bb56-427c-fd4f-195871339e64",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2000,loss:0.179780\n",
      "Epoch:4000,loss:0.025656\n",
      "Epoch:6000,loss:0.006857\n",
      "Epoch:8000,loss:0.003678\n",
      "Epoch:10000,loss:0.002464\n",
      "Epoch:12000,loss:0.001836\n",
      "Epoch:14000,loss:0.001456\n",
      "Epoch:16000,loss:0.001203\n",
      "Epoch:18000,loss:0.001023\n",
      "Epoch:20000,loss:0.000889\n",
      "[(array([0, 0]), np.float64(0.03)), (array([0, 1]), np.float64(0.97)), (array([1, 0]), np.float64(0.97)), (array([1, 1]), np.float64(0.03))]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "def mean_square_error_loss(y_true,y_pred):\n",
    "  return np.mean((y_true - y_pred)**2) # Use subtraction for MSE\n",
    "\n",
    "input = np.array([[0,0],[0,1],[1,0],[1,1]]) # Correct the input array format\n",
    "output = np.array([[0],[1],[1],[0]]) # Correct the output array format\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "\n",
    "weights_input_hidden = np.random.rand(input_size,hidden_size)\n",
    "bias_hidden = np.random.rand(hidden_size)\n",
    "weights_hidden_output = np.random.randn(hidden_size,output_size)\n",
    "bias_output = np.random.rand(output_size)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  hidden_input = np.dot(input,weights_input_hidden)+bias_hidden\n",
    "  hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "  final_input = np.dot(hidden_output,weights_hidden_output)+bias_output\n",
    "  final_output = sigmoid(final_input)\n",
    "\n",
    "  loss = mean_square_error_loss(output,final_output)\n",
    "\n",
    "  error_output = final_output - output # Fix variable name 'outputs'\n",
    "  gradient_output = error_output*sigmoid_derivative(final_output)\n",
    "\n",
    "  error_hidden = np.dot(gradient_output,weights_hidden_output.T)\n",
    "  gradient_hidden = error_hidden*sigmoid_derivative(hidden_output)\n",
    "\n",
    "  weights_hidden_output -= learning_rate*np.dot(hidden_output.T,gradient_output)\n",
    "  bias_output -= learning_rate*np.sum(gradient_output,axis=0)\n",
    "  weights_input_hidden -= learning_rate*np.dot(input.T,gradient_hidden)\n",
    "  bias_hidden -= learning_rate*np.sum(gradient_hidden,axis=0)\n",
    "\n",
    "  if (epoch + 1)%2000 == 0: # Correct indentation\n",
    "    print(f\"Epoch:{epoch + 1},loss:{loss:.6f}\")\n",
    "\n",
    "results = [] # Fix variable name 'result'\n",
    "for input_pair in input: # Fix variable name 'inputs', iterate over 'input'\n",
    "  hidden_input = np.dot(input_pair,weights_input_hidden)+bias_hidden\n",
    "  hidden_output = sigmoid(hidden_input)\n",
    "  final_input = np.dot(hidden_output,weights_hidden_output)+bias_output\n",
    "  final_output = sigmoid(final_input)\n",
    "  results.append((input_pair,np.round(final_output[0],2))) # Append a tuple\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJa0_WFj9aBd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq1E_MurAAnx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWi4Zi8HAAx6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "UAub3iLHBb8l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix 1:\n",
      "[[0.59547254 0.04492351 0.7603022 ]\n",
      " [0.27728983 0.0222163  0.40370252]]\n",
      "Weight matrix 2:\n",
      "[[0.31167435]\n",
      " [0.77696074]\n",
      " [0.18740206]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNet(object):\n",
    "    RNG = np.random.default_rng()\n",
    "\n",
    "    def __init__(self, topology:list[int] = []):\n",
    "        self.topology = topology\n",
    "        self.weight_mats = []\n",
    "        self._init_matrices()\n",
    "\n",
    "    def _init_matrices(self):\n",
    "        #-- set up matrices\n",
    "        if len(self.topology) > 1:\n",
    "            j = 1\n",
    "            for i in range(len(self.topology)-1):\n",
    "                num_rows = self.topology[i]\n",
    "                num_cols = self.topology[j] # Dedent this line to align with num_rows\n",
    "                mat = self.RNG.random(size=(num_rows, num_cols)) # Dedent to match the for loop level\n",
    "                self.weight_mats.append(mat)\n",
    "                j += 1\n",
    "\n",
    "    # ... rest of the code ...\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNet(object):\n",
    "    RNG = np.random.default_rng()\n",
    "\n",
    "    def __init__(self, topology: list[int] = []):\n",
    "        self.topology = topology\n",
    "        self.weight_mats = []\n",
    "        self._init_matrices()\n",
    "\n",
    "    def _init_matrices(self):\n",
    "        if len(self.topology) > 1:\n",
    "            j = 1\n",
    "            for i in range(len(self.topology) - 1):\n",
    "                num_rows = self.topology[i]\n",
    "                num_cols = self.topology[j]\n",
    "                mat = self.RNG.random(size=(num_rows, num_cols))\n",
    "                self.weight_mats.append(mat)\n",
    "                j += 1\n",
    "\n",
    "# Create an instance of the NeuralNet class\n",
    "nn = NeuralNet([2, 3, 1])\n",
    "\n",
    "# Display the initialized weight matrices\n",
    "for i, weights in enumerate(nn.weight_mats):\n",
    "    print(f\"Weight matrix {i+1}:\")\n",
    "    print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ncLWeRZRAA0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights and biases:\n",
      "Weight matrix 1:\n",
      "[[0.10600272 0.84346438]\n",
      " [0.72373789 0.5849808 ]]\n",
      "Weight matrix 2:\n",
      "[[0.81279157]\n",
      " [0.71880256]]\n",
      "Bias 1:\n",
      "[0.72824983 0.61388377]\n",
      "Bias 2:\n",
      "[0.51166429]\n",
      "Epoch: 2000, Loss: 0.231420\n",
      "Epoch: 4000, Loss: 0.141978\n",
      "Epoch: 6000, Loss: 0.014081\n",
      "Epoch: 8000, Loss: 0.005337\n",
      "Epoch: 10000, Loss: 0.003155\n",
      "Epoch: 12000, Loss: 0.002208\n",
      "Epoch: 14000, Loss: 0.001686\n",
      "Epoch: 16000, Loss: 0.001359\n",
      "Epoch: 18000, Loss: 0.001135\n",
      "Epoch: 20000, Loss: 0.000973\n",
      "\n",
      "Final weights and biases:\n",
      "Weight matrix 1:\n",
      "[[4.13333613 6.17152128]\n",
      " [4.14711672 6.23622533]]\n",
      "Weight matrix 2:\n",
      "[[-9.24486744]\n",
      " [ 8.52667391]]\n",
      "Bias 1:\n",
      "[-6.35167332 -2.691138  ]\n",
      "Bias 2:\n",
      "[-3.88327035]\n",
      "\n",
      "Predicted outputs for XOR inputs:\n",
      "[(array([0, 0]), np.float64(0.03)), (array([0, 1]), np.float64(0.97)), (array([1, 0]), np.float64(0.97)), (array([1, 1]), np.float64(0.03))]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def mean_square_error_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)  # Mean Squared Error Loss\n",
    "\n",
    "# Neural Network class definition\n",
    "class NeuralNet(object):\n",
    "    RNG = np.random.default_rng()\n",
    "\n",
    "    def __init__(self, topology: list[int] = []):\n",
    "        self.topology = topology\n",
    "        self.weight_mats = []\n",
    "        self.biases = []\n",
    "        self._init_matrices()\n",
    "\n",
    "    def _init_matrices(self):\n",
    "        # Initialize weight matrices and biases for each layer connection\n",
    "        if len(self.topology) > 1:\n",
    "            for i in range(len(self.topology) - 1):\n",
    "                num_rows = self.topology[i]\n",
    "                num_cols = self.topology[i + 1]\n",
    "                mat = self.RNG.random(size=(num_rows, num_cols))\n",
    "                bias = self.RNG.random(size=(num_cols,))\n",
    "                self.weight_mats.append(mat)\n",
    "                self.biases.append(bias)\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        activations = inputs\n",
    "        for weights, bias in zip(self.weight_mats, self.biases):\n",
    "            activations = sigmoid(np.dot(activations, weights) + bias)\n",
    "        return activations\n",
    "\n",
    "    def train(self, inputs, outputs, learning_rate=0.1, epochs=20000):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            hidden_input = np.dot(inputs, self.weight_mats[0]) + self.biases[0]\n",
    "            hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "            final_input = np.dot(hidden_output, self.weight_mats[1]) + self.biases[1]\n",
    "            final_output = sigmoid(final_input)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = mean_square_error_loss(outputs, final_output)\n",
    "\n",
    "            # Backpropagation\n",
    "            error_output = final_output - outputs\n",
    "            gradient_output = error_output * sigmoid_derivative(final_output)\n",
    "\n",
    "            error_hidden = np.dot(gradient_output, self.weight_mats[1].T)\n",
    "            gradient_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weight_mats[1] -= learning_rate * np.dot(hidden_output.T, gradient_output)\n",
    "            self.biases[1] -= learning_rate * np.sum(gradient_output, axis=0)\n",
    "\n",
    "            self.weight_mats[0] -= learning_rate * np.dot(inputs.T, gradient_hidden)\n",
    "            self.biases[0] -= learning_rate * np.sum(gradient_hidden, axis=0)\n",
    "\n",
    "            # Print the loss every 2000 epochs\n",
    "            if (epoch + 1) % 2000 == 0:\n",
    "                print(f\"Epoch: {epoch + 1}, Loss: {loss:.6f}\")\n",
    "\n",
    "    def print_weights(self):\n",
    "        for i, weights in enumerate(self.weight_mats):\n",
    "            print(f\"Weight matrix {i + 1}:\")\n",
    "            print(weights)\n",
    "        for i, bias in enumerate(self.biases):\n",
    "            print(f\"Bias {i + 1}:\")\n",
    "            print(bias)\n",
    "\n",
    "# Create and train the neural network\n",
    "nn = NeuralNet([2, 2, 1])  # 2 neurons in input, 2 in hidden, 1 in output\n",
    "\n",
    "# Define input and output data for XOR\n",
    "input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "output_data = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Print initial weights and biases\n",
    "print(\"Initial weights and biases:\")\n",
    "nn.print_weights()\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(input_data, output_data, learning_rate=0.1, epochs=20000)\n",
    "\n",
    "# Print final weights and biases after training\n",
    "print(\"\\nFinal weights and biases:\")\n",
    "nn.print_weights()\n",
    "\n",
    "# Test the trained network on the XOR inputs\n",
    "results = []\n",
    "for input_pair in input_data:\n",
    "    prediction = nn.forward_pass(input_pair)\n",
    "    results.append((input_pair, np.round(prediction[0], 2)))\n",
    "\n",
    "print(\"\\nPredicted outputs for XOR inputs:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ts7vtNyDAA3p",
    "outputId": "40808769-8b2f-468b-cf9c-b3cefa7e27d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Old weights (w1):\n",
      "[[0.2 0.4 0.1]\n",
      " [0.5 0.3 0.2]\n",
      " [0.3 0.7 0.8]]\n",
      "\n",
      "Old biases (b1):\n",
      "[0.1 0.2 0.3]\n",
      "\n",
      "Old weights (w2):\n",
      "[[0.6 0.4 0.5]\n",
      " [0.1 0.2 0.3]\n",
      " [0.3 0.7 0.2]]\n",
      "\n",
      "old biases (b2):\n",
      "[0.1 0.2 0.3]\n",
      "[ 0.25502746 -0.58413061  0.32910315]\n",
      "[0.25502746 0.41586939 0.32910315]\n",
      "0.8773840448515435\n"
     ]
    }
   ],
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([0.8, 0.6, 0.7])\n",
    "y = np.array([0, 1, 0])\n",
    "w1 = np.array([[0.2, 0.4, 0.1],\n",
    "              [0.5, 0.3, 0.2],\n",
    "              [0.3, 0.7, 0.8]])\n",
    "w2 = np.array([[0.6, 0.4, 0.5],\n",
    "              [0.1, 0.2, 0.3],\n",
    "              [0.3, 0.7, 0.2]])\n",
    "b1 = np.array([0.1, 0.2, 0.3])\n",
    "b2 = np.array([0.1, 0.2, 0.3])\n",
    "print(f\"\\nOld weights (w1):\\n{w1}\")\n",
    "print(f\"\\nOld biases (b1):\\n{b1}\")\n",
    "print(f\"\\nOld weights (w2):\\n{w2}\")\n",
    "print(f\"\\nold biases (b2):\\n{b2}\")\n",
    "\n",
    "# alpha\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "  return 1*(x>0)\n",
    "\n",
    "def softmax(A):\n",
    "    exps = np.exp(A - np.max(A, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "# Loss = summation(y_true * log(y_pred))\n",
    "def lossFunction(y_true,y_pred):\n",
    "  return -np.sum(y_true*np.log(y_pred))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  # Forward propagation\n",
    "\n",
    "  # h1 = w1(x)+b1\n",
    "  h1 = np.dot(x, w1)+b1\n",
    "  # a1 = sig(h1)\n",
    "  a1 = relu(h1)\n",
    "\n",
    "  # h2 = w2(a1)+b2\n",
    "  h2 = np.dot(a1, w2)+b2\n",
    "  # a2 = softmax(h2)\n",
    "  a2 = softmax(h2)\n",
    "\n",
    "  # L = (h2-y)^2\n",
    "  loss = lossFunction(y, a2)\n",
    "  #error = a2 - y\n",
    "  #Error = (a2 - y)\n",
    "  error = a2 - y\n",
    "\n",
    "print(error)\n",
    "print(a2)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyQCF_VwOpmO",
    "outputId": "c30e6b4e-ae56-4565-913d-9e928e6deaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10000, Loss: 0.8774\n",
      "\n",
      "Final outputs (a2):\n",
      " [0.25502746 0.41586939 0.32910315]\n",
      "Final Loss:\n",
      " 0.8773840448515435\n",
      "\n",
      "Updated weights (w1):\n",
      "[[0.20115017 0.40115017 0.10115017]\n",
      " [0.50115017 0.30115017 0.20115017]\n",
      " [0.30115017 0.70115017 0.80115017]]\n",
      "\n",
      "Updated biases (b1):\n",
      "[0.10058413 0.20058413 0.30058413]\n",
      "\n",
      "Updated weights (w2):\n",
      "[[0.60149895 0.40149895 0.50149895]\n",
      " [0.10149895 0.20149895 0.30149895]\n",
      " [0.30149895 0.70149895 0.20149895]]\n",
      "\n",
      "Updated biases (b2):\n",
      "[0.1 0.2 0.3]\n",
      "\n",
      "Output (a2) after epoch 10000:\n",
      "[0.25502746 0.41586939 0.32910315]\n"
     ]
    }
   ],
   "source": [
    "  # d(L)/d(w2) = Error * a1\n",
    "  d_L_d_w2 = error.dot(a1.T)\n",
    "\n",
    "  # d(L)/d(b2) = Error\n",
    "  d_L_d_b2 = np.mean(error, axis=0)\n",
    "\n",
    "  d_L_d_hidden = np.dot(error, w2.T)\n",
    "  d_hidden_d_input = relu_derivative(h1)\n",
    "\n",
    "  # d(L)/d(w1) = Error * w2 * x\n",
    "  d_L_d_w1 = np.dot(x.T, d_hidden_d_input * d_L_d_hidden)\n",
    "\n",
    "  # d(L)/d(b1) = Error * w2\n",
    "  d_L_d_b1 = np.mean(d_hidden_d_input * d_L_d_hidden, axis=0)\n",
    "\n",
    "\n",
    "  # Update weights\n",
    "  w2 -= learning_rate * d_L_d_w2\n",
    "  b2 -= learning_rate * d_L_d_b2\n",
    "\n",
    "  w1 -= learning_rate *d_L_d_w1\n",
    "  b1 -= learning_rate * d_L_d_b1\n",
    "\n",
    "  if (epoch + 1) % 1000 == 0:\n",
    "    print(f\"Epoch:{epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# Print final outputs\n",
    "print(\"\\nFinal outputs (a2):\\n\", a2)\n",
    "print(\"Final Loss:\\n\", loss)\n",
    "\n",
    "results = []\n",
    "# for input_pair in inputs:\n",
    "#           hidden_input = np.dot(input_pair, weights_input_hidden)+bias_hidden\n",
    "#           hidden_output = sigmoid(hidden_input)\n",
    "#           final_input = np.dot(hidden_output, weights_hidden_output)+bias_output\n",
    "#           final_output = sigmoid(final_input)\n",
    "#           results.append((input_pair,np.round(final_output[0],2)))\n",
    "# print(results)\n",
    "\n",
    "print(f\"\\nUpdated weights (w1):\\n{w1}\")\n",
    "print(f\"\\nUpdated biases (b1):\\n{b1}\")\n",
    "print(f\"\\nUpdated weights (w2):\\n{w2}\")\n",
    "print(f\"\\nUpdated biases (b2):\\n{b2}\")\n",
    "print(f\"\\nOutput (a2) after epoch {epoch + 1}:\\n{a2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
